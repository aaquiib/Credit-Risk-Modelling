{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "md-01",
   "metadata": {},
   "source": ["# 03 — Model Training & Evaluation\n", "Train 9 classifiers and compare performance on train and test sets."]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, GradientBoostingClassifier,\n",
    "    AdaBoostClassifier, ExtraTreesClassifier\n",
    ")\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    ")\n",
    "\n",
    "from src.preprocessing import load_and_clean, build_preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-setup",
   "metadata": {},
   "source": ["## 1. Data Setup"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_and_clean()\n",
    "\n",
    "X = df.drop('Risk', axis=1)\n",
    "y = df['Risk'].map({'good': 1, 'bad': 0})\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "print(f'Train: {X_train.shape}  |  Test: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-models",
   "metadata": {},
   "source": ["## 2. Define Models"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27496e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Logistic'        : LogisticRegression(),\n",
    "    'RandomForest'    : RandomForestClassifier(),\n",
    "    'SVM'             : SVC(),\n",
    "    'DecisionTree'    : DecisionTreeClassifier(),\n",
    "    'AdaBoost'        : AdaBoostClassifier(),\n",
    "    'GradientBoosting': GradientBoostingClassifier(),\n",
    "    'XGBoost'         : XGBClassifier(eval_metric='logloss'),\n",
    "    'LightGBM'        : LGBMClassifier(),\n",
    "    'ExtraTrees'      : ExtraTreesClassifier(),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-train",
   "metadata": {},
   "source": ["## 3. Train & Evaluate"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0a7588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_performance(name, tr_acc, tr_f1, tr_prec, tr_rec, tr_auc,\n",
    "                      te_acc, te_f1, te_prec, te_rec, te_auc):\n",
    "    print(name)\n",
    "    print('  Train  — Acc: {:.4f}  F1: {:.4f}  Prec: {:.4f}  Rec: {:.4f}  AUC: {}'.format(\n",
    "        tr_acc, tr_f1, tr_prec, tr_rec,\n",
    "        f'{tr_auc:.4f}' if tr_auc is not None else 'N/A'))\n",
    "    print('  Test   — Acc: {:.4f}  F1: {:.4f}  Prec: {:.4f}  Rec: {:.4f}  AUC: {}'.format(\n",
    "        te_acc, te_f1, te_prec, te_rec,\n",
    "        f'{te_auc:.4f}' if te_auc is not None else 'N/A'))\n",
    "    print('=' * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12798a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = build_preprocessor()\n",
    "\n",
    "train_results = {}\n",
    "test_results  = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    pipe = Pipeline([('prep', preprocessor), ('model', model)])\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    y_tr_pred = pipe.predict(X_train)\n",
    "    y_te_pred = pipe.predict(X_test)\n",
    "\n",
    "    y_tr_prob = pipe.predict_proba(X_train)[:, 1] if hasattr(pipe, 'predict_proba') else None\n",
    "    y_te_prob = pipe.predict_proba(X_test)[:, 1]  if hasattr(pipe, 'predict_proba') else None\n",
    "\n",
    "    tr_acc  = accuracy_score(y_train, y_tr_pred)\n",
    "    tr_prec = precision_score(y_train, y_tr_pred)\n",
    "    tr_rec  = recall_score(y_train, y_tr_pred)\n",
    "    tr_f1   = f1_score(y_train, y_tr_pred)\n",
    "    tr_auc  = roc_auc_score(y_train, y_tr_prob) if y_tr_prob is not None else None\n",
    "\n",
    "    te_acc  = accuracy_score(y_test, y_te_pred)\n",
    "    te_prec = precision_score(y_test, y_te_pred)\n",
    "    te_rec  = recall_score(y_test, y_te_pred)\n",
    "    te_f1   = f1_score(y_test, y_te_pred)\n",
    "    te_auc  = roc_auc_score(y_test, y_te_prob) if y_te_prob is not None else None\n",
    "\n",
    "    print_performance(name, tr_acc, tr_f1, tr_prec, tr_rec, tr_auc,\n",
    "                            te_acc, te_f1, te_prec, te_rec, te_auc)\n",
    "\n",
    "    train_results[name] = dict(Train_Accuracy=tr_acc, Train_Precision=tr_prec,\n",
    "                               Train_Recall=tr_rec, Train_F1=tr_f1, Train_ROC_AUC=tr_auc)\n",
    "    test_results[name]  = dict(Test_Accuracy=te_acc,  Test_Precision=te_prec,\n",
    "                               Test_Recall=te_rec,  Test_F1=te_f1,  Test_ROC_AUC=te_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-summary",
   "metadata": {},
   "source": ["## 4. Results Summary"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355783ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train Summary (sorted by Recall):')\n",
    "pd.DataFrame(train_results).T.sort_values('Train_Recall', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f3ec90",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test Summary (sorted by Recall):')\n",
    "pd.DataFrame(test_results).T.sort_values('Test_Recall', ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
